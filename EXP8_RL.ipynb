{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPWMOAqM94wgfBwx3UuvWJ9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ThunDroid1/Ujjwal_RL/blob/main/EXP8_RL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 366
        },
        "id": "CAZKTXEk3Csz",
        "outputId": "f9863d10-d064-4a22-9d21-ac0e133d601e"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-b969a9bb4db4>\u001b[0m in \u001b[0;36m<cell line: 27>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;31m# Run Value Iteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m     \u001b[0moptimal_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimal_policy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransitions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Optimal Value Function:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-b969a9bb4db4>\u001b[0m in \u001b[0;36mvalue_iteration\u001b[0;34m(transitions, rewards, gamma, epsilon)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;31m# Update the value function using the Bellman optimality equation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mQ_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meinsum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ijk, ij->i'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransitions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mgamma\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mV\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mV\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mQ_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (3,2) (3,) "
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "def value_iteration(transitions, rewards, gamma=0.9, epsilon=1e-6):\n",
        "    num_states, num_actions, _ = transitions.shape\n",
        "\n",
        "    # Initialize value function\n",
        "    V = np.zeros(num_states)\n",
        "\n",
        "    while True:\n",
        "        # Store the current value function for convergence check\n",
        "        V_prev = np.copy(V)\n",
        "\n",
        "        # Update the value function using the Bellman optimality equation\n",
        "        Q_values = np.einsum('ijk, ij->i', transitions, rewards + gamma * V)\n",
        "        V = np.max(Q_values, axis=1)\n",
        "\n",
        "        # Check for convergence\n",
        "        if np.max(np.abs(V - V_prev)) < epsilon:\n",
        "            break\n",
        "\n",
        "    # Find the optimal policy based on the computed value function\n",
        "    optimal_policy = np.argmax(np.einsum('ijk,ij->i', transitions, rewards + gamma * V), axis=1)\n",
        "\n",
        "    return V, optimal_policy\n",
        "\n",
        "# Example usage:\n",
        "if __name__ == \"__main__\":\n",
        "    # Example MDP with 3 states and 2 actions\n",
        "    num_states = 3\n",
        "    num_actions = 2\n",
        "\n",
        "    # Define transition probabilities and rewards\n",
        "    transitions = np.array([[[0.7, 0.3, 0.0], [0.1, 0.6, 0.3]],\n",
        "                            [[0.0, 0.8, 0.2], [0.4, 0.4, 0.2]],\n",
        "                            [[0.5, 0.5, 0.0], [0.0, 0.0, 1.0]]])\n",
        "\n",
        "    rewards = np.array([[1, 0], [0, 0], [-1, 0]])\n",
        "\n",
        "    # Run Value Iteration\n",
        "    optimal_values, optimal_policy = value_iteration(transitions, rewards)\n",
        "\n",
        "    print(\"Optimal Value Function:\")\n",
        "    print(optimal_values)\n",
        "\n",
        "    print(\"\\nOptimal Policy:\")\n",
        "    print(optimal_policy)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def value_iteration(states, actions, transition_probabilities, rewards, gamma=0.9):\n",
        "    value_table = {}\n",
        "    policy = {}\n",
        "\n",
        "    for state in states:\n",
        "        value_table[state] = 0\n",
        "        policy[state] = np.random.choice(actions)\n",
        "\n",
        "    while True:\n",
        "        delta = 0\n",
        "        for state in states:\n",
        "            old_value = value_table[state]\n",
        "            action_value = float('-inf')\n",
        "\n",
        "            for action in actions:\n",
        "                prob = transition_probabilities[state][action]\n",
        "                reward = rewards[state][action]\n",
        "                # Assuming value_table is a dictionary\n",
        "                action_value = max(action_value, sum(prob[next_state] * (reward[next_state] + gamma * value_table[next_state]) for next_state in states))\n",
        "\n",
        "            value_table[state] = action_value\n",
        "            delta = max(delta, abs(old_value - value_table[state]))\n",
        "\n",
        "        if delta < 1e-10:\n",
        "            break\n",
        "\n",
        "    return value_table, policy\n",
        "\n",
        "# Test the function\n",
        "states = [0, 1, 2, 3]\n",
        "actions = ['up', 'down']\n",
        "transition_probabilities = {\n",
        "    0: {'up': [0, 1, 0], 'down': [0, 0, 1]},\n",
        "    1: {'up': [0, 1, 0], 'down': [0, 0, 1]},\n",
        "    2: {'up': [0, 1, 0], 'down': [0, 0, 1]},\n",
        "    3: {'up': [0, 0, 1], 'down': [0, 0, 1]}\n",
        "}\n",
        "rewards = {\n",
        "    0: {'up': [0, 0, 0], 'down': [0, 0, 0]},\n",
        "    1: {'up': [0, 0, 0], 'down': [0, 0, 0]},\n",
        "    2: {'up': [0, 0, 0], 'down': [0, 0, 0]},\n",
        "    3: {'up': [0, 0, 0], 'down': [0, 0, 0]}\n",
        "}\n",
        "\n",
        "value_table, policy = value_iteration(states, actions, transition_probabilities, rewards)\n",
        "\n",
        "print(\"Value Table:\")\n",
        "for state, value in value_table.items():\n",
        "    print(f\"{state}: {value}\")\n",
        "\n",
        "print(\"\\nPolicy:\")\n",
        "for state, action in policy.items():\n",
        "    print(f\"{state}: {action}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        },
        "id": "_CM6AYrc661s",
        "outputId": "f0a1bcb2-285b-486b-ab2e-d3bd42a14bea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-8b7b7c714ed2>\u001b[0m in \u001b[0;36m<cell line: 47>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     45\u001b[0m }\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m \u001b[0mvalue_table\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransition_probabilities\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Value Table:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-8b7b7c714ed2>\u001b[0m in \u001b[0;36mvalue_iteration\u001b[0;34m(states, actions, transition_probabilities, rewards, gamma)\u001b[0m\n\u001b[1;32m     19\u001b[0m                 \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m                 \u001b[0;31m# Assuming value_table is a dictionary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m                 \u001b[0maction_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprob\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mgamma\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mvalue_table\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mnext_state\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0mvalue_table\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maction_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-8b7b7c714ed2>\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     19\u001b[0m                 \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m                 \u001b[0;31m# Assuming value_table is a dictionary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m                 \u001b[0maction_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprob\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mgamma\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mvalue_table\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mnext_state\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0mvalue_table\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maction_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: list index out of range"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6RUYHc7B9NXh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}